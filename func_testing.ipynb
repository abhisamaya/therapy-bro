{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0deff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # loads OPENAI_API_KEY from .env\n",
    "client = OpenAI()\n",
    "\n",
    "def stream_chat(messages, model=\"gpt-4o-mini\", **kwargs):\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "      {\"role\":\"system\",\"content\":\"You are concise.\"},\n",
    "      {\"role\":\"user\",\"content\":\"Explain streams briefly.\"}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "    for chunk in resp:\n",
    "        if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
    "            yield chunk.choices[0].delta.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf06760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Growth in Popularity**: Streaming services have seen exponential growth, with platforms like Netflix, Amazon Prime, and Disney+ amassing millions of subscribers worldwide, significantly impacting traditional cable TV viewership.\n",
      "\n",
      "2. **Content Variety**: Streaming platforms offer a diverse range of content, including movies, TV shows, documentaries, and original programming, catering to various audience preferences and demographics.\n",
      "\n",
      "3. **Accessibility**: Streaming allows users to access content on multiple devices, such as smartphones, tablets, smart TVs, and computers, providing flexibility in viewing anytime and anywhere with an internet connection.\n"
     ]
    }
   ],
   "source": [
    "msgs = [\n",
    "        {\"role\": \"system\", \"content\": \"You are concise.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 facts about streaming.\"},\n",
    "    ]\n",
    "for token in stream_chat(msgs, temperature=0.3):\n",
    "    print(token, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a5a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Iterable, List, Literal, Dict, Any, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- env / client ------------------------------------------------------------\n",
    "load_dotenv()  # expects OPENAI_API_KEY in .env or environment\n",
    "client = OpenAI()\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    id: str\n",
    "    messages: List[Message]\n",
    "\n",
    "\n",
    "# --- JSON storage ------------------------------------------------------------\n",
    "class JsonStore:\n",
    "    def __init__(self, root: str = \"./chats\") -> None:\n",
    "        self.root = root\n",
    "        os.makedirs(self.root, exist_ok=True)\n",
    "\n",
    "    def _path(self, cid: str) -> str:\n",
    "        return os.path.join(self.root, f\"{cid}.json\")\n",
    "\n",
    "    def save(self, conv: Conversation) -> None:\n",
    "        data = {\n",
    "            \"id\": conv.id,\n",
    "            \"messages\": [asdict(m) for m in conv.messages],\n",
    "        }\n",
    "        with open(self._path(conv.id), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load(self, conversation_id: str) -> Optional[Conversation]:\n",
    "        path = self._path(conversation_id)\n",
    "        if not os.path.isfile(path):\n",
    "            return None\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = json.load(f)\n",
    "        return Conversation(\n",
    "            id=raw[\"id\"],\n",
    "            messages=[Message(**m) for m in raw.get(\"messages\", [])],\n",
    "        )\n",
    "\n",
    "\n",
    "# --- OpenAI streaming --------------------------------------------------------\n",
    "def stream_chat(messages: List[Dict[str, str]],\n",
    "                model: str = \"gpt-4o-mini\",\n",
    "                **kwargs) -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    Yield assistant text deltas (tokens/fragments) as they arrive.\n",
    "    `messages` is a list of dicts [{role, content}, ...].\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "    for chunk in resp:\n",
    "        if chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if delta and delta.content:\n",
    "                yield delta.content\n",
    "\n",
    "\n",
    "# --- High-level helper to send a message, stream, and persist ----------------\n",
    "def send_and_stream_with_persist(store: JsonStore,\n",
    "                                 conversation_id: Optional[str],\n",
    "                                 user_text: str,\n",
    "                                 system_if_new: Optional[str] = None,\n",
    "                                 model: str = \"gpt-4o-mini\",\n",
    "                                 **kwargs) -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    - If `conversation_id` is None or not found → create new conversation (uuid).\n",
    "      Optionally bootstrap with a system message (`system_if_new`).\n",
    "    - Append the user's message.\n",
    "    - Stream assistant reply; yield chunks.\n",
    "    - Save the final assistant message and updated history to JSON.\n",
    "    Returns a generator of chunks; also returns `conversation_id` via attribute.\n",
    "    \"\"\"\n",
    "    # Load or create conversation\n",
    "    conv = None\n",
    "    if conversation_id:\n",
    "        conv = store.load(conversation_id)\n",
    "\n",
    "    if conv is None:\n",
    "        conversation_id = uuid.uuid4().hex\n",
    "        conv = Conversation(id=conversation_id, messages=[])\n",
    "        if system_if_new:  # optional system bootstrap\n",
    "            conv.messages.append(Message(role=\"system\", content=system_if_new))\n",
    "        store.save(conv)  # persist immediately so the file exists\n",
    "\n",
    "    # Append user message and persist\n",
    "    conv.messages.append(Message(role=\"user\", content=user_text))\n",
    "    store.save(conv)\n",
    "\n",
    "    # Build wire messages for OpenAI\n",
    "    wire = [{\"role\": m.role, \"content\": m.content} for m in conv.messages]\n",
    "\n",
    "    # Stream assistant reply\n",
    "    assembled: List[str] = []\n",
    "    try:\n",
    "        for tok in stream_chat(wire, model=model, **kwargs):\n",
    "            assembled.append(tok)\n",
    "            yield tok\n",
    "    finally:\n",
    "        # Persist assistant message even if stream/error interrupted\n",
    "        full = \"\".join(assembled)\n",
    "        conv.messages.append(Message(role=\"assistant\", content=full))\n",
    "        store.save(conv)\n",
    "\n",
    "# Convenience alias on the generator to access the id during iteration:\n",
    "class StreamWithId:\n",
    "    def __init__(self, gen: Iterable[str], conversation_id: str) -> None:\n",
    "        self._gen = gen\n",
    "        self.conversation_id = conversation_id\n",
    "    def __iter__(self):\n",
    "        return iter(self._gen)\n",
    "\n",
    "\n",
    "def start_streaming_message(store: JsonStore,\n",
    "                            user_text: str,\n",
    "                            conversation_id: Optional[str] = None,\n",
    "                            system_if_new: Optional[str] = None,\n",
    "                            model: str = \"gpt-4o-mini\",\n",
    "                            **kwargs) -> StreamWithId:\n",
    "    \"\"\"\n",
    "    Wrapper that returns a generator *plus* exposes conversation_id immediately.\n",
    "    \"\"\"\n",
    "    # Peek/create the conversation id consistently with the helper above\n",
    "    if conversation_id and store.load(conversation_id) is not None:\n",
    "        cid = conversation_id\n",
    "    else:\n",
    "        cid = conversation_id or uuid.uuid4().hex\n",
    "    gen = send_and_stream_with_persist(\n",
    "        store=store,\n",
    "        conversation_id=cid,\n",
    "        user_text=user_text,\n",
    "        system_if_new=system_if_new,\n",
    "        model=model,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return StreamWithId(gen, cid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f2147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation ID: 62d8365610f742b9866db20f1c593487\n",
      "---\n",
      "Certainly! Step 2 refers to the concept of **Bandwidth and Quality** in streaming, which is crucial for delivering a seamless viewing or listening experience. Here’s a more detailed explanation:\n",
      "\n",
      "### Bandwidth and Quality in Streaming\n",
      "\n",
      "1. **Understanding Bandwidth**: Bandwidth is the maximum rate at which data can be transmitted over an internet connection. It is typically measured in megabits per second (Mbps). The higher the bandwidth, the more data can be sent and received simultaneously. This is particularly important for streaming, as video and audio files can be large and require significant data transfer.\n",
      "\n",
      "2. **Impact on Streaming Quality**: The quality of the streamed content—such as resolution and clarity—depends heavily on the available bandwidth. For example:\n",
      "   - **Standard Definition (SD)** video typically requires about 3 Mbps.\n",
      "   - **High Definition (HD)** video (720p) requires around 5-8 Mbps.\n",
      "   - **Full HD (1080p)** video can require 10-15 Mbps.\n",
      "   - **4K Ultra HD** video demands significantly more bandwidth, often needing 25 Mbps or higher.\n",
      "\n",
      "3. **Adaptive Streaming**: Many streaming services use a technology called adaptive bitrate streaming. This means that the service can automatically adjust the quality of the video stream based on the user's current internet speed. If the connection is strong, the service will deliver higher-quality video. Conversely, if the connection weakens, the service will lower the quality to prevent buffering (interruptions in playback). This ensures a smoother experience for the user, even if their internet speed fluctuates.\n",
      "\n",
      "4. **Buffering and User Experience**: When the bandwidth is insufficient to support the desired video quality, users may experience buffering, which is when the video pauses to load more data. This can be frustrating and detracts from the overall viewing experience. To mitigate this, users are often encouraged to have a stable and fast internet connection, especially when streaming high-definition content.\n",
      "\n",
      "In summary, bandwidth plays a critical role in determining the quality of streaming content. A higher bandwidth allows for better quality streaming, while adaptive streaming technologies help maintain a smooth experience even when internet speeds vary.\n",
      "--- (assistant message persisted)\n"
     ]
    }
   ],
   "source": [
    "store = JsonStore(\"./chats\")\n",
    "\n",
    "stream1 = start_streaming_message(\n",
    "        store,\n",
    "        user_text=\"Explain step 2 \",\n",
    "        conversation_id=\"62d8365610f742b9866db20f1c593487\",  # new convo\n",
    "        system_if_new=\"you are concise.\",\n",
    "        temperature=0.3,\n",
    "    )\n",
    "print(f\"\\nConversation ID: {stream1.conversation_id}\\n---\")\n",
    "for chunk in stream1:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n--- (assistant message persisted)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8449f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemyllms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
